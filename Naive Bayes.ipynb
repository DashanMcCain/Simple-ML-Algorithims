{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A probabalistic classifier based on Bayes' theorem with a strong (naive) independence assumptions between the features\n",
    "\n",
    "- Bayes' Theorem states that the probability of event A given event B is equal to the probability of B given A times the probability of A all over the probability of B\n",
    "\n",
    "- In the classification case, the Theorem states that the probability of a class label given its feature vector is equal to the probability of the feature vector's given the class label, times the probability of the class label all over the probability of the feature vector. This is based on the assumption that the features are mutually independent. \n",
    "\n",
    "- This assumption is usually not the case for real-world examples, hence the name naive. When applying Naive Bayes, the formula above is prone to errors since the probabilities are between 0 and 1, meaning that across n iterations when n is large, the prediction can go to 0. Therefore, we use the prior probability (frequency of each class) and class conditional probability (model with gaussian distribution) to calculate the posterior probability\n",
    "\n",
    "- When training, calculate mean, variance, and prior frequency for each class. For predictions, calculate the posterior probability for each class with the frequency of each class and the gaussian formula, then choose the class with the highest posterior probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # Calculate mean, variance, and prior for each class\n",
    "        self._mean = np.zeros((n_classes, n_features),dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features),dtype=np.float64)\n",
    "        self._priors = np.zeros((n_classes),dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[idx,:] = X_c.mean(axis=0)\n",
    "            self._var[idx,:] = X_c.var(axis=0)\n",
    "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, X):\n",
    "        posteriors = []\n",
    "\n",
    "        # Calculate posterior probability for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, X)))\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "            # Return class with the highest posterior\n",
    "            return self._classes[np.argmax(posteriors)]\n",
    "        \n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(-((x- mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.495\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "X, y = datasets.make_classification (\n",
    "    n_samples =1000, n_features=10, n_classes=2, random_state=123\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.fit(X_train, y_train)\n",
    "predictions = nb.predict(X_test)\n",
    "\n",
    "print(accuracy(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2428ae8bed9cd321a058f75c5910092ede82b77977e4ba5d4d5fac6d1f70533"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
